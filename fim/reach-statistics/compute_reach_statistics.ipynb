{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e98bdb-54c7-4d9b-bb5a-e158c1ea4bdc",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to compute reach statistics based on retrospective model predictions.\n",
    "\n",
    "**Requirements**  \n",
    "- hsclient  \n",
    "- S3hsclient\n",
    "- pandas\n",
    "- pyarrow\n",
    "- s3fs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d4d9c9-96d9-40a9-bc8d-63c7e960a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas\n",
    "import S3hsclient as hsclient\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8513b5-0bd9-41d3-bb59-b4543d71c108",
   "metadata": {},
   "source": [
    "Historical streamflow data has been collected and stored in HydroShare at: [https://www.hydroshare.org/resource/446999d3f27149cea4ee9863d2de7ad2/](https://www.hydroshare.org/resource/446999d3f27149cea4ee9863d2de7ad2/) . We can access these data via HydroShare's S3 integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60f25f9-6914-4bb9-8cdf-8d99eb856eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username:  TonyCastronova\n",
      "Password for TonyCastronova:  ········\n"
     ]
    }
   ],
   "source": [
    "hs = hsclient.S3HydroShare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb337c8-37c7-477e-8127-a3f6fc1b39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = hs.resource('446999d3f27149cea4ee9863d2de7ad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f78be5-ed93-493d-9877-e7310df6f801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tonycastronova/446999d3f27149cea4ee9863d2de7ad2/data/contents/DeSoto.parquet',\n",
       " 'tonycastronova/446999d3f27149cea4ee9863d2de7ad2/data/contents/MountAscutney.parquet',\n",
       " 'tonycastronova/446999d3f27149cea4ee9863d2de7ad2/data/contents/RoaringRiver.parquet',\n",
       " 'tonycastronova/446999d3f27149cea4ee9863d2de7ad2/data/contents/SpringfieldGreeneCounty.parquet',\n",
       " 'tonycastronova/446999d3f27149cea4ee9863d2de7ad2/data/contents/TwoRiversOttauquechee.parquet',\n",
       " 'tonycastronova/446999d3f27149cea4ee9863d2de7ad2/data/contents/Windham.parquet']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.s3_ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5180d71-49aa-4b5b-926f-42711c6e93a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_parquet('tonycastronova/446999d3f27149cea4ee9863d2de7ad2/data/contents/DeSoto.parquet', engine='pyarrow', filesystem=res._hs_session.s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf2ea7d-9082-4dc6-97d7-7e701e78e0fc",
   "metadata": {},
   "source": [
    "Compute quantiles for all reaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f41bf-1774-4d6f-9d89-48ba438d2cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TwoRiversOttauquechee..."
     ]
    }
   ],
   "source": [
    "for region_name in ['DeSoto',\n",
    "                    'MountAscutney',\n",
    "                    'RoaringRiver',\n",
    "                    'SpringfieldGreeneCounty',\n",
    "                    'TwoRiversOttauquechee',\n",
    "                    'Windham'\n",
    "                    ]:\n",
    "    st = time.time()\n",
    "    print(f'Processing {region_name}...', end='', flush=True)\n",
    "    df = pandas.read_parquet(f'tonycastronova/446999d3f27149cea4ee9863d2de7ad2/data/contents/{region_name}.parquet',\n",
    "                             engine='pyarrow',\n",
    "                             filesystem=res._hs_session.s3)\n",
    "    \n",
    "    quantile_levels = [0, 0.5, 0.10, 0.25, 0.75, 0.90, 1.0]\n",
    "    quantile_names = ['q0', 'q5', 'q10', 'q25', 'q75', 'q90', 'q100']\n",
    "    \n",
    "    # make sure 'doy' exists\n",
    "    df['doy'] = df['time'].dt.dayofyear\n",
    "    \n",
    "    # compute quantiles grouped by feature_id and doy\n",
    "    quantiles = (\n",
    "        df.groupby(['feature_id', 'doy'])['streamflow']\n",
    "          .quantile(quantile_levels)\n",
    "          .unstack()  # quantiles become columns\n",
    "          .rename(columns=dict(zip(quantile_levels, quantile_names)))\n",
    "          .reset_index()\n",
    "    )\n",
    "    \n",
    "    quantiles.to_parquet(f'{region_name}_quantiles.parquet')\n",
    "\n",
    "    print(f'done. [{time.time() - st} sec elapsed]')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59dcfc1-32b5-4534-ab71-d5a20958ca8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29828962-7007-4237-978b-27a706aec824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b541c77-6784-4797-b8b3-50b3fe9eea16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53715d20-b21b-4b65-8fc3-2428a896cf60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca17a0db-afd8-44e1-b12d-85f0e33cf281",
   "metadata": {},
   "source": [
    "Speed this up using dask (this isn't faster on my local computer, probably need more resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e493bd-2cde-4ee8-a0d6-2c31d3876254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69c6a64d-ca70-4370-b796-33df7ccd1f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "# use a try accept loop so we only instantiate the client\n",
    "# if it doesn't already exist.\n",
    "try:\n",
    "    print(client.dashboard_link)\n",
    "except:    \n",
    "    # The client should be customized to your workstation resources.\n",
    "    # This is configured for a \"Large\" instance on ciroh.awi.2i2c.cloud\n",
    "    # client = Client()\n",
    "    client = Client(\n",
    "                    n_workers=2,\n",
    "                    memory_limit='8GB',\n",
    "                    local_directory=\"./dask-worker-space\" # where to spill data \n",
    "    )\n",
    "    print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6475c00d-0955-4823-a368-a22b4b598c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 85.9 ms, sys: 27.7 ms, total: 114 ms\n",
      "Wall time: 758 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>feature_id</th>\n",
       "      <th>streamflow</th>\n",
       "      <th>doy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: assign, 4 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                         time feature_id streamflow    doy\n",
       "npartitions=3                                             \n",
       "               datetime64[ns]      int64    float64  int32\n",
       "                          ...        ...        ...    ...\n",
       "                          ...        ...        ...    ...\n",
       "                          ...        ...        ...    ...\n",
       "Dask Name: assign, 4 expressions\n",
       "Expr=Assign(frame=ReadParquetFSSpec(b993b2f))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7978ce4-2a5d-402c-af3f-610599d3ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "region = 'DeSoto'\n",
    "\n",
    "ddf = dd.read_parquet(f'tonycastronova/446999d3f27149cea4ee9863d2de7ad2/data/contents/{region}.parquet',\n",
    "                      filesystem=res._hs_session.s3)\n",
    "ddf['doy'] = ddf['time'].dt.dayofyear\n",
    "ddf\n",
    "\n",
    "N_CHUNKS = 8   \n",
    "\n",
    "# Get sorted unique feature_ids\n",
    "feature_ids = ddf['feature_id'].unique().compute()\n",
    "feature_ids = np.sort(feature_ids)\n",
    "\n",
    "# Split feature_ids into N_CHUNKS\n",
    "chunk_size = math.ceil(len(feature_ids) / N_CHUNKS)\n",
    "feature_id_chunks = [feature_ids[i:i+chunk_size] for i in range(0, len(feature_ids), chunk_size)]\n",
    "\n",
    "# Folder for temporary parquet files\n",
    "tmp_dir = \"tmp_quantiles\"\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "quantile_levels = [0.0, 0.5, 0.10, 0.25, 0.75, 0.90, 1.0]\n",
    "quantile_names = ['q0', 'q5', 'q10', 'q25', 'q75', 'q90', 'q100']\n",
    "\n",
    "def compute_quantiles(subdf):\n",
    "    q = subdf['streamflow'].quantile(quantile_levels)\n",
    "    q.index = quantile_names\n",
    "    return pandas.Series(q)\n",
    "\n",
    "# Process each chunk\n",
    "output_files = []\n",
    "for i, ids_chunk in enumerate(feature_id_chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(feature_id_chunks)} with {len(ids_chunk)} feature_ids...\")\n",
    "\n",
    "    ddf_chunk = ddf[ddf['feature_id'].isin(ids_chunk)]\n",
    "    quantiles_chunk = (\n",
    "        ddf_chunk.groupby(['feature_id', 'doy'])\n",
    "                 .apply(compute_quantiles,\n",
    "                        meta={name: 'f8' for name in quantile_names},\n",
    "                        include_groups=False)\n",
    "    ).compute()\n",
    "\n",
    "    quantiles_chunk.reset_index(inplace=True)\n",
    "    quantiles_chunk.columns.name = None\n",
    "\n",
    "    outfile = os.path.join(tmp_dir, f\"quantiles_part_{i}.parquet\")\n",
    "    quantiles_chunk.to_parquet(outfile) #, write_index=False)\n",
    "    output_files.append(outfile)\n",
    "\n",
    "# Merge all chunks into a final file\n",
    "print(\"Merging all parquet chunks...\")\n",
    "ddf_merged = dd.read_parquet(output_files)\n",
    "df = ddf_merged.reset_index().compute\n",
    "\n",
    "final_file = f\"{region}_quantiles.parquet\"\n",
    "df.to_parquet(final_file, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c5e66-70ca-446f-8da1-5faffc8c8207",
   "metadata": {},
   "source": [
    "## Detect Good and Bad Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76716ffe-b481-4b84-8291-eb59c25e95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_quantile_collapse(d, threshold=0.01):\n",
    "    d.set_index('doy', inplace=True)\n",
    "    diff = d.q75 - d.q25\n",
    "    diff.loc[diff < threshold] = 1\n",
    "    diff.loc[diff != 1] = 0\n",
    "    return pandas.Series({'collapsed': sum(diff)})\n",
    "    \n",
    "regions = ['DeSoto', 'RoaringRiver', 'SpringfieldGreeneCounty', 'Windham', 'MountAscutney', 'TwoRiversOttauquechee']\n",
    "dfs = []\n",
    "for region in regions:\n",
    "    df = pandas.read_parquet(f'{region}_quantiles.parquet', engine='pyarrow')\n",
    "    \n",
    "    # compute collapse scores\n",
    "    scores = df.groupby('feature_id').apply(detect_quantile_collapse, include_groups=False)\n",
    "    scores.reset_index(inplace=True)\n",
    "    \n",
    "    # drop reaches with too many collapses\n",
    "    fids = scores[scores.collapsed <= 50].feature_id.dropna()\n",
    "    good_data = df.where(df.feature_id.isin(fids)).dropna()\n",
    "\n",
    "    dfs.append(good_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1ddf0cc-894b-4767-bae0-15b07551a8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "366\n",
      "366\n"
     ]
    }
   ],
   "source": [
    "for d in dfs:\n",
    "    print(len(d[d.feature_id == 9328724]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8635f37c-b313-49a2-92be-99e41d323f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data frames and save the result to parquet\n",
    "all_data = pandas.concat(dfs, ignore_index=True).drop(columns=['index'])\n",
    "all_data.feature_id = all_data.feature_id.astype(int)\n",
    "\n",
    "# drop any duplicates that might exist. This could happen if region boundaries \n",
    "# overlap slightly. A unique record is determined by combining 'feature_id' and 'doy'\n",
    "all_data.drop_duplicates(subset=['feature_id', 'doy'], keep='first', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cac649b-ef87-49cf-a771-a6049bfcc1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 reaches containing NaN records\n"
     ]
    }
   ],
   "source": [
    "# drop any reach that contains NaNs\n",
    "null_mask = all_data.isnull().any(axis=1)\n",
    "nulls = all_data[null_mask]\n",
    "null_reaches = nulls.feature_id.unique()\n",
    "print(f'Found {len(null_reaches)} reaches containing NaN records')\n",
    "\n",
    "if len(null_reaches) > 0:\n",
    "    print('Dropping Reaches containing NaN')\n",
    "    all_data = all_data[~all_data.feature_id.isin(null_reaches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07f29e76-c593-49fa-bf23-1b56d1106014",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_parquet(f'all_quantiles_good.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e6ae8-6ca4-41d6-aad1-02a44bbe5813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
