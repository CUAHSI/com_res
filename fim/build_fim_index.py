#!/usr/bin/env python


"""
The purpose of this script is to automate the process of creating a 
STAC (SpatioTemporal Asset Catalog) for the FIM maps that are generated by the FIM service.
These maps are stored in Google Cloud Storage (GCS).
"""

import os
import typer
import pystac
import sqlite3
import pandas as pd
from pathlib import Path
from typing import Optional
from datetime import datetime
from google.cloud import storage
from google.cloud import bigquery
from rich.progress import (
    Progress,
    BarColumn,
    TextColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)


app = typer.Typer()


def upload_directory_to_gcs_with_progress(local_dir, bucket_name, gcs_prefix=""):
    client = storage.Client()
    bucket = client.bucket(bucket_name)

    # Collect all files to upload
    file_list = []
    for root, _, files in os.walk(local_dir):
        for file_name in files:
            local_path = os.path.join(root, file_name)
            relative_path = os.path.relpath(local_path, local_dir)
            blob_path = os.path.join(gcs_prefix, relative_path).replace("\\", "/")
            file_list.append((local_path, blob_path))

    # Show progress bar
    with Progress(
        TextColumn("[bold blue]{task.fields[filename]}", justify="right"),
        BarColumn(),
        "[progress.percentage]{task.percentage:>3.0f}%",
        "•",
        TransferSpeedColumn(),
        "•",
        TimeRemainingColumn(),
    ) as progress:

        task = progress.add_task("Uploading", total=len(file_list), filename="")

        for local_path, blob_path in file_list:
            progress.update(task, filename=os.path.basename(local_path))
            blob = bucket.blob(blob_path)
            blob.upload_from_filename(local_path)
            progress.advance(task)

def upload_single_file_to_gcs(local_file, bucket_name, gcs_prefix=""):
    client = storage.Client()
    bucket = client.bucket(bucket_name)

    dest_blob_name =  os.path.join(gcs_prefix, Path(local_file).name)
    
    blob = bucket.blob(dest_blob_name)
    blob.upload_from_filename(local_file)
    print(f"Uploaded to gs://{bucket_name}/{dest_blob_name}")
    

def flatten_stac_items(catalog_path: str) -> pd.DataFrame:
    root_catalog = pystac.Catalog.from_file(str(Path(catalog_path) / "catalog.json"))
    
    rows = []
    for item in root_catalog.get_all_items():
        
        row = {
            "item_id": item.id,
            "reach_id": item.properties.get("id"),
            "stage": item.properties.get("stage"),
            "flow": item.properties.get("flow"),
            "datetime": item.datetime.isoformat() if item.datetime else None,
            "asset_href": item.assets["cog"].href if "cog" in item.assets else None,
            "public_url": item.assets['cog'].href.replace('gs://','https://storage.googleapis.com/')
        }
        rows.append(row)

    return pd.DataFrame(rows)

def load_stac_to_bigquery(gcs_uri: str, table_id: str):
    client = bigquery.Client()

    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
        autodetect=True,
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
    )

    load_job = client.load_table_from_uri(gcs_uri, table_id, job_config=job_config)
    load_job.result()  # Wait for job to complete
    print(f"Loaded {load_job.output_rows} rows to {table_id}")

@app.command(name="build_index")
def build_index(
    gcs_bucket: str = typer.Argument(..., help="Name of the GCS bucket."),
    prefix: str = typer.Option(
        '', help="Prefix (like a folder path) to search under."
    ),
    extension: str = typer.Option(
        ".cog", help="File extension to filter by (e.g., .tif, .csv)."
    ),
    output_dir: Path = typer.Option('fim-catalog',
                                     help='Name of the output directory for the STAC catalog.'
                                     ),
):
    """
    Creates the index for the FIM maps that are stored in the Google Cloud Storage. The operation performs the following operations: 
    
    1. Creates a STAC catalog
    
    2. Creates a flattened JSONL file of STAC entries
    
    3. Uploads both to GCS
    
    4. Loads the JSONL into BigQuery
    """

    # Connect to the GCS bucket and all the files matching the extension "cog" in
    # the all subdirectories. Save these as a list of paths.
    print(
        f"Gathering COG files in {gcs_bucket} with prefix '{prefix}' and extension '{extension}'...",
        end="",
    )
    client = storage.Client()
    bucket = client.bucket(gcs_bucket)
    blobs = client.list_blobs(bucket, prefix=prefix)
    matching_files = []
    for blob in blobs:
        if blob.name.endswith(extension):
            matching_files.append(f"gs://{gcs_bucket}/{blob.name}")
    print("done")

    # organize the matching files by reach id
    # extract FIM attributes and save them for building
    # that catalog later on.
    print(
        "Extracting Metadata from FIM Maps...",
        end="",
    )
    items = {}
    for url in matching_files:
        filename_parts = url.split('/')[-1].split('__')
        item_id = url.split('/')[-1].replace(extension, "")
        reach_id = int(filename_parts[0])
        stage = float('.'.join(filename_parts[1].split('_')[0:2]))
        flow = float(filename_parts[2].split('_')[0])
    
        dat = {'url': url,
               'item_id': item_id,
               'reach_id': reach_id,
               'stage': stage,
               'flow': flow}
        if reach_id not in items.keys():
            items[reach_id] = [dat]
        else:
            items[reach_id].append(dat)
    print('done')
    
    print(
        "Constructing SpatioTemporal Asset Catalog...",
        end="",
    )
    catalog = pystac.Catalog(
        id="fim-data-catalog", description="Flood maps indexed by id, stage, and flow"
    )
    
    # For each reach_id, create a sub-catalog and add items
    # this is done to speed up querying.
    for reach_id, items_for_id in items.items():
        sub_catalog = pystac.Catalog(id=f"reach-{reach_id}", description=f"Catalog for reach_id {reach_id}")
        catalog.add_child(sub_catalog)

        # create stac entries for each of the maps and set
        # FIM attributes to enable querying later.
        for attrs in items_for_id:
            
            stac_item = pystac.Item(
                id=attrs['item_id'],
                geometry=None,
                bbox=None,
                datetime=datetime(
                    1970, 1, 1, 0, 0, 0
                ),  # placeholder because this parameter is required
                properties={"id": attrs['reach_id'], "stage": attrs['stage'], "flow": attrs['flow']},
                )

            # TODO: reference the GeoTiff too, but we'll need to make sure it exists in the cloud.
            stac_item.add_asset(
                "cog",
                pystac.Asset(
                    href=attrs['url'],
                    media_type=pystac.MediaType.COG,
                    roles=["data"],
                    title="Cloud Optimized GeoTiff",
                ),
            )
        
            sub_catalog.add_item(stac_item)
    

        # Save the sub-catalog to its folder
        sub_catalog_dir = os.path.join(output_dir, f"reach-{reach_id}")
        os.makedirs(sub_catalog_dir, exist_ok=True)
        sub_catalog.normalize_and_save(root_href=sub_catalog_dir,
                                       catalog_type=pystac.CatalogType.SELF_CONTAINED)

    # Save the root catalog
    outpath_str = str(output_dir.absolute())
    catalog.normalize_and_save(root_href=outpath_str, catalog_type=pystac.CatalogType.SELF_CONTAINED)
    print("done")
    
    print(f"STAC catalog saved to: {outpath_str}")

    print(f"Flattening STAC entries into a JSONL file...", end='')
    df = flatten_stac_items("fim-catalog")
    df.to_json("fim_catalog_index.jsonl", orient="records", lines=True)
    print('done')

    print(f"Uploading JSONL to GCP...")
    upload_single_file_to_gcs("fim_catalog_index.jsonl", "com_res_fim_output")
    

    print("Uploading STAC catalog to GCS bucket...")
    upload_directory_to_gcs_with_progress(outpath_str, gcs_bucket, gcs_prefix="catalog")

    print("Loading catalog into BigQuery...")
    load_stac_to_bigquery(f"gs://{gcs_bucket}/fim_catalog_index.jsonl", "com-res.flood_data.fim_catalog")


if __name__ == "__main__":
    app()
